# -*- coding: utf-8 -*-
"""sd_inpainting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rWtk1Bw3d0nf25vuglU5MYyaGnAsZgrZ
"""

import torch
print(torch.cuda.is_available())

# Module 1 - Clearing env and installing libraries - to run on A100
import os
import torch

print("Running Module 0: Organising env...\n")

_xformers_version        = "0.0.23.post1"
_transformers_version    = "4.37.2"
_diffusers_version       = "0.25.0"
_accelerate_version      = "0.26.1"
_huggingface_hub_version = "0.20.3"
_peft_version            = "0.8.2"

_facenet_pytorch_version = "2.5.2"
_tensorflow_version      = "2.15.0"
_keras_version           = "2.15.0"

# Remove all potentially conflicting packages first
print("Uninstalling conflicting packages...")
!pip uninstall -y torch torchvision torchaudio xformers diffusers transformers accelerate peft huggingface_hub facenet-pytorch deepface lpips scikit-image opencv-python

# Tensorflow and Keras
print(f"\nInstalling TensorFlow {_tensorflow_version} & Keras {_keras_version}...")
!pip install --no-cache-dir tensorflow=={_tensorflow_version} keras=={_keras_version}

# Deepface
print("\nInstalling DeepFace...")
!pip install --no-cache-dir deepface

# PyTorch, CUDA 12.1, matching torchvision/torchaudio
print("\nInstalling PyTorch, torchvision and torchaudio…")
!pip install --no-cache-dir \
    torch==2.1.2+cu121 \
    torchvision==0.16.2+cu121 \
    torchaudio==2.1.2+cu121 \
    --index-url https://download.pytorch.org/whl/cu121

# HF diffusers, PEFT, xFormers
print(f"\nInstalling Diffusers {_diffusers_version}, Transformers {_transformers_version}, Accelerate {_accelerate_version}, Hub {_huggingface_hub_version}, PEFT {_peft_version}, xFormers {_xformers_version}…")
!pip install --no-cache-dir \
    diffusers=={_diffusers_version} \
    transformers=={_transformers_version} \
    accelerate=={_accelerate_version} \
    huggingface_hub=={_huggingface_hub_version} \
    peft=={_peft_version} \
    xformers=={_xformers_version} \
    safetensors

# --- 6) Install other face/vision libraries ---
print(f"\nInstalling Facenet-Pytorch {_facenet_pytorch_version}, Mediapipe, LPIPS, scikit-image, OpenCV…")
!pip install --no-cache-dir \
    facenet-pytorch=={_facenet_pytorch_version} \
    mediapipe \
    lpips \
    scikit-image \
    opencv-python \
    Pillow \
    tqdm \
    numpy \
    pandas \
    matplotlib

# --- 7) Verify Dependencies ---
print("\nVerifying installed package versions…")
!pip check

# Check imports have been successfully installed
print("\nVerifying imports...")

import torch


torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32       = True
torch.backends.cudnn.benchmark        = True

print("Successfully installed and imported libraries: \n")
print("pytorch imported")
import torchvision; print("torchvision imported")
import torchaudio; print("torchaudio imported")
import xformers; print("xformers imported")
import diffusers; print("diffusers imported")
import transformers; print("transformers imported")
import accelerate; print("accelerate imported")
import huggingface_hub; print("HF hub imported")
import peft; print("peft imported")
import tensorflow; print("tensorflow imported")
import keras; print("keras imported")
from deepface import DeepFace; print("deepface imported")
import facenet_pytorch; print("facenet-pytorch imported")
import PIL; print("pillow imported")
import numpy; print("numpy imported")
import safetensors; print("safetensors imported")
import mediapipe; print("mediapipe imported")
from torchvision import transforms; print("transforms imported")
import lpips; print("lpips imported")
import skimage; print("skimage imported")
import sys
from PIL import ImageFont; print("imagefont imported")
from torchvision import transforms; print("transforms imported")

import logging
logging.getLogger().setLevel(logging.WARNING)




import os, torch
import google.colab
from google.colab import drive

# mount grdive
drive.mount("/content/drive", force_remount=True)

# config folder

CONFIG = {
    "drive_base": "/content/drive/MyDrive/fairface_folder",
    "laion_zip_drive_path": "/content/drive/MyDrive/combo_data/good_input_snippets/laion_input_snippets_1_1.zip",
    "model_weights_folder": "/content/drive/MyDrive/fairface_folder/model_weights",
    "model_save_name": "stable_diffusion_inpaint_lora.safetensors",
    "local_laion_folder": "/content/laion_face_images_input", # temporary local folder to download "laion_zip_drive_path" into
    "local_output_folder": "/content/inpainted_results_output", # temporary local folder to copy inpainting outputs to
    "halfdone_zip" : "/content/halfdone_output_snippet_1_1.zip",
    # for ^^halfdone_zip, directly upload an unfinished output zip of "local_laion_folder", and the code will resume it for you
    "hf_model_name" : "runwayml/stable-diffusion-inpainting",
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "lora_rank": 32,
    "lora_alpha": 32,
    "image_size": 516,
    "guidance_scale": 7.5,
    "num_inference_steps": 40,
    "seed": 42,
    "min_face_ratio": 0.001,
    "min_confidence": 0.90,
    "shiny_highlight_threshold": 240, # pixel-value above which is "too bright" -> frequent issue in inpainting
    "shiny_pixel_ratio":       0.02, # fraction of pixels above that threshold
    "rerank_shiny_candidates": 3,

    # reranking configs
    "num_candidates": 4, # number of seeds per prompt
    "clip_weight": 0.4, # CLIP weight for scoring
    "lpips_weight": 0.2, # LPIPS weight for scoring
    "ssim_weight": 0.2, # SSIM weight for scoring
    "land_weight": 0.2, # landmark similarity score weight for scoring
    "landmark_yaw_threshold": 100, # max absolute yaw to use landmark metric
    "landmark_min_pts": 10, # minimum visible points for landmark metric
    "landmark_vis_threshold": 0.5, # mediapipe landmark threshold
    "generation_batch_size": 40,

}
config = CONFIG


# Make sure folders exist
os.makedirs(CONFIG["local_laion_folder"], exist_ok=True)
os.makedirs(CONFIG["local_output_folder"], exist_ok=True)



print("CONFIG and GDrive ready.")

# check for "laion_zip_drive_path" on gdrive and unzip into "local_laion_folder"
import os
laion_zip = CONFIG["laion_zip_drive_path"]

print("Looking for:", laion_zip)
print("Exists? ", os.path.exists(laion_zip))
print("Parent dir listing:", os.listdir(os.path.dirname(laion_zip)))

if os.path.exists(laion_zip):
    import zipfile
    target_dir = CONFIG["local_laion_folder"]
    os.makedirs(target_dir, exist_ok=True)
    print(f"Unzipping into {target_dir!r}…")
    with zipfile.ZipFile(laion_zip, 'r') as z:
        z.extractall(target_dir)
    print("✅ Unzip complete.")
else:
    raise FileNotFoundError(f"Could not find `{laion_zip}`. Check your path and Drive mount.")

# Module 1: LoRA Injections

import math
import re
import torch
import torch.nn as nn
import numpy as np
from PIL import Image
from facenet_pytorch import MTCNN
from deepface import DeepFace
from safetensors.torch import load_file as safeload

# LoRA Conv2D Injection
class LoRAInjectConv2d(nn.Module):
    def __init__(self, base: nn.Conv2d, rank=4, alpha=1.0):
        super().__init__()
        self.base = base
        self._dtype = base.weight.dtype
        self._device = base.weight.device

        self.rank = rank
        self.scale = alpha / max(rank, 1)

        # up and down adapters
        self.down = nn.Conv2d(
            base.in_channels, rank,
            kernel_size=1, stride=1, padding=0, bias=False,
            dtype=self._dtype, device=self._device
        )
        self.up = nn.Conv2d(
            rank, base.out_channels,
            kernel_size=1, stride=1, padding=0, bias=False,
            dtype=self._dtype, device=self._device
        )

        nn.init.kaiming_uniform_(self.down.weight, a=math.sqrt(5))
        nn.init.zeros_(self.up.weight)

        self.base.weight.requires_grad = False
        if self.base.bias is not None:
            self.base.bias.requires_grad = False

    def forward(self, x):
        main = self.base(x)
        lora = self.up(self.down(x)) * self.scale
        return main + lora.to(main.dtype)


# LoRA Linear Injection
class LoRAInjectLinear(nn.Module):
    def __init__(self, base: nn.Linear, rank=4, alpha=1.0):
        super().__init__()
        self.base = base
        self._dtype = base.weight.dtype
        self._device = base.weight.device

        self.rank = rank
        self.scale = alpha / max(rank, 1)

        self.down = nn.Linear(
            base.in_features, rank, bias=False,
            dtype=self._dtype, device=self._device
        )
        self.up = nn.Linear(
            rank, base.out_features, bias=False,
            dtype=self._dtype, device=self._device
        )

        nn.init.kaiming_uniform_(self.down.weight, a=math.sqrt(5))
        nn.init.zeros_(self.up.weight)

        self.base.weight.requires_grad = False
        if self.base.bias is not None:
            self.base.bias.requires_grad = False

    def forward(self, x):
        main = self.base(x)
        lora = self.up(self.down(x)) * self.scale
        return main + lora.to(main.dtype)

# Module 2: Helper Functions


def replace_with_lora(module, rank, alpha, base_path=""):
   # Wraps LoRA conv2d and linear modules with their corresponding adapters
    wrapped_children = False
    for name, child in module.named_children():
        path = f"{base_path}.{name}" if base_path else name
        print(f"  [Debug] Considering: {path} (Type: {type(child).__name__})")

        child_wrapped = replace_with_lora(child, rank, alpha, path)
        wrapped_children = wrapped_children or child_wrapped

        if isinstance(child, nn.Conv2d):
            if tuple(child.kernel_size) == (1, 1):
                try:
                    wrapped = LoRAInjectConv2d(child, rank, alpha)
                    setattr(module, name, wrapped)
                    print(f"✅ Injected LoRA into 1×1 Conv at: {path}")
                    wrapped_children = True
                except Exception as e:
                    print(f"❌ ERROR injecting Conv LoRA at {path}: {e}")
            else:
                 print(f"  [Debug] Skipping Conv2d at {path}: Kernel size {child.kernel_size} != (1, 1)")

        elif isinstance(child, nn.Linear):
             try:
                 wrapped = LoRAInjectLinear(child, rank, alpha)
                 setattr(module, name, wrapped)
                 print(f"✅ Injected LoRA into Linear at: {path}")
                 wrapped_children = True
             except Exception as e:
                 print(f"❌ ERROR injecting Linear LoRA at {path}: {e}")

    return wrapped_children


# load weights
def load_file(path, device="cpu"):
    if path.endswith(".safetensors"):
        return safeload(path, device=device)
    return torch.load(path, map_location=device)


# load LoRA state dict into unet
def load_lora_state_dict(module, state_dict, model_name="UNet", strict=False):
    # load LoRA weights from state_dict to unet
    print(f"🔍 Mapping LoRA keys for {model_name}...")
    renamed = {}
    skipped_not_lora = 0
    skipped_resolve = 0
    skipped_cast_error = 0
    processed_keys = set()
    state_dict_tensor_keys = [k for k, v in state_dict.items() if isinstance(v, torch.Tensor)]
    print(f"   - Found {len(state_dict_tensor_keys)} tensor keys in the state_dict file.")

    for key, value in state_dict.items():
        if not isinstance(value, torch.Tensor):
            continue
        processed_keys.add(key)

        parts = key.split(".")
        if len(parts) < 2 or parts[-1] not in ("lora_down", "lora_up"):
            continue

        param_type = "down" if parts[-1] == "lora_down" else "up"
        layer_path = ".".join(parts[:-1])
        target_name = f"{layer_path}.{param_type}.weight"

        try:
            ptr = module
            for sub in layer_path.split("."):
                ptr = getattr(ptr, sub)

            if isinstance(ptr, (LoRAInjectConv2d, LoRAInjectLinear)):
                target_param = getattr(ptr, param_type).weight
                target_dtype = target_param.dtype

                try:
                    casted_value = value.to(target_dtype)
                    renamed[target_name] = casted_value
                except Exception as cast_e:
                    print(f"      ❌ Cast Error: Failed to cast {key} from {value.dtype} to {target_dtype}. Error: {cast_e}")
                    skipped_cast_error += 1

            else:
                print(f"      ⚠️ Skipped NOT LORA MODULE: {key} (module at {layer_path} is type {type(ptr)})")
                skipped_not_lora += 1

        except AttributeError:
            skipped_resolve += 1
        except Exception as e:
            print(f"      ⚠️ Skipped UNEXPECTED ERROR for key {key}: {e}")

    print(f"\n🧩 Attempting to load {len(renamed)} mapped & casted tensors into {model_name}...")
    if skipped_cast_error > 0:
         print(f"   - Skipped {skipped_cast_error} keys due to errors during dtype casting.")
    if skipped_not_lora > 0:
         print(f"   - Skipped {skipped_not_lora} keys because the target module wasn't a LoRA wrapper.")
    if skipped_resolve > 0:
         print(f"   - Skipped {skipped_resolve} keys because the module path/parameter couldn't be resolved.")


    result = module.load_state_dict(renamed, strict=False)

    loaded = len(renamed)
    print(f"✅ Successfully loaded: {loaded} tensors")

    # report missing keys
    if result.missing_keys:
        print(f"   ⚠️ Missing keys in model (expected LoRA params not found/mapped from file): {len(result.missing_keys)}")

    # report unexpeted keys
    if result.unexpected_keys:
        print(f"   ⚠️ Unexpected keys (mapped from file but not found in model - check mapping logic): {len(result.unexpected_keys)}")

    # final sanity check
    if len(renamed) == 0 and len(state_dict_tensor_keys) > 0 and skipped_cast_error == 0:
        print("\n❌ CRITICAL WARNING: No LoRA keys were successfully mapped. Check key names in file vs. model structure.")
    elif loaded == 0 and len(renamed) > 0:
        print("\n❌ CRITICAL WARNING: Mapping occurred, but load_state_dict failed to load any tensors. Check unexpected_keys.")

    return result


# load MTCNN for face detection
mtcnn_detector = MTCNN(
    keep_all=True,
    thresholds=[0.5, 0.6, 0.6],
    device=CONFIG["device"]
)
print(f"✅ MTCNN ready on {CONFIG['device']}")

def estimate_age(pil_img):
    # return age-group string after using deepface to detect age
    # correlates to the same age groups specified in the FairFace dataset
    try:
        arr = np.array(pil_img.convert("RGB"))[:, :, ::-1]
        res = DeepFace.analyze(arr, actions=["age"], enforce_detection=False)
        age = (res[0]["age"] if isinstance(res, list) else res["age"])
    except Exception:
        return "adult"

    if age <= 2:   return "baby"
    if age <= 9:   return "child"
    if age <= 19:  return "teenager"
    if age <= 39:  return "young adult"
    if age <= 59:  return "middle-aged adult"
    return "senior adult"


def batchify(iterable, batch_size):
    if batch_size < 1:
        raise ValueError("batch_size must be >= 1")
    it = iter(iterable)
    while True:
        chunk = tuple(itertools.islice(it, batch_size))
        if not chunk:
            return
        yield chunk



import numpy as np
from PIL import Image
from typing import Tuple

def is_shiny_patch(
    img: Image.Image,
    box: Tuple[int,int,int,int],
    highlight_th: int,
    pixel_ratio_th: float
) -> bool:
    x1,y1,x2,y2 = box
    patch = img.crop((x1,y1,x2,y2)).convert("L")
    arr   = np.array(patch)
    shiny_pixels = (arr > highlight_th).sum()
    return (shiny_pixels / arr.size) > pixel_ratio_th



import inspect, huggingface_hub, diffusers
print("Batchify done! :D")
print("shiny function done")


print("huggingface_hub location:", inspect.getfile(huggingface_hub))
print("diffusers location:", inspect.getfile(diffusers))
print("huggingface_hub version:", huggingface_hub.__version__)
print("diffusers version:", diffusers.__version__)

print("\nModule 2 Helper Functions defined.")

# Module 3: Loading Pipelines - verbose

import copy
from diffusers import StableDiffusionInpaintPipeline, DPMSolverMultistepScheduler
from safetensors.torch import load_file as safeload

assert "CONFIG" in globals(), "Run Module 1 first."
assert all(k in globals() for k in ("replace_with_lora", "load_file", "load_lora_state_dict")), "Run Module 2 first."

# load base
print("🔄 Loading base pipeline...")
pipe_base = StableDiffusionInpaintPipeline.from_pretrained(
    CONFIG["hf_model_name"],
    torch_dtype=torch.float16 if CONFIG["device"] == "cuda" else torch.float32,
    safety_checker=None,
    feature_extractor=None
).to(CONFIG["device"])
pipe_base.unet = pipe_base.unet.to(memory_format=torch.channels_last)
pipe_base.vae  = pipe_base.vae.to (memory_format=torch.channels_last)




try:
    pipe_base.enable_xformers_memory_efficient_attention()
    print("✅ xFormers enabled on base pipeline")
except:
    pass

pipe_base.scheduler = DPMSolverMultistepScheduler.from_config(pipe_base.scheduler.config)
print("✅ pipe_base ready (no LoRA)")

# show all unet keys
print("🔍 Sample base UNet params:")
for i, (name, _) in enumerate(pipe_base.unet.named_parameters()):
    print(f"  {i:02d}: {name}")
    if i >= 30: break



# obtain target LoRA layers
lora_path = os.path.join(CONFIG["model_weights_folder"], CONFIG["model_save_name"])
assert os.path.exists(lora_path), f"❌ LoRA checkpoint not found at: {lora_path}"
print(f"📦 Identifying target layers from: {lora_path}")
state_dict_lora_file = load_file(lora_path, device="cpu")

lora_target_module_paths = set()
for key in state_dict_lora_file.keys():
    if not isinstance(state_dict_lora_file[key], torch.Tensor):
      continue

    parts = key.split('.')
    if len(parts) > 2 and parts[-1] in ("weight", "bias"):
         if parts[-2] in ("lora_down", "lora_up"):
             module_path = ".".join(parts[:-2])
             lora_target_module_paths.add(module_path)
    elif len(parts) > 1 and parts[-1] in ("lora_down", "lora_up"):
         module_path = ".".join(parts[:-1])
         lora_target_module_paths.add(module_path)


print(f"Identified {len(lora_target_module_paths)} target module paths from LoRA file.")
print("Loading base pipeline...")
print("Cloning base pipeline for LoRA...")
pipe_lora = copy.deepcopy(pipe_base)

# inject LoRA modules into unet
replace_with_lora(pipe_lora.unet, rank=CONFIG["lora_rank"], alpha=CONFIG["lora_alpha"])
print(f"✅ LoRA adapters injected (rank={CONFIG['lora_rank']}, alpha={CONFIG['lora_alpha']})")

# inject and load the LoRA weights
lora_path = os.path.join(CONFIG["model_weights_folder"], CONFIG["model_save_name"])
assert os.path.exists(lora_path), f"❌ LoRA checkpoint not found at: {lora_path}"
print(f"📦 Loading LoRA checkpoint from: {lora_path}")

state_dict = load_file(lora_path, device=CONFIG["device"])
load_lora_state_dict(pipe_lora.unet, state_dict, model_name="UNet")

pipe_lora.unet = pipe_lora.unet.to(memory_format=torch.channels_last)
pipe_lora.vae  = pipe_lora.vae.to (memory_format=torch.channels_last)
pipe_lora.unet = torch.compile(pipe_lora.unet)



try:
    pipe_lora.enable_xformers_memory_efficient_attention()
    print("xformers enabled on LoRA pipeline")
except:
    pass


print("cloning base pipeline for LoRA...")
pipe_lora = copy.deepcopy(pipe_base)

print("\nstarting LoRA Injection...")
replace_with_lora(pipe_lora.unet, rank=CONFIG["lora_rank"], alpha=CONFIG["lora_alpha"])
print(f"LoRA adapter injection process complete.")


# sanity check
wrapped_layers = sum(
    isinstance(m, (LoRAInjectConv2d, LoRAInjectLinear))
    for _, m in pipe_lora.unet.named_modules()
)
print(f"🔎 Found {wrapped_layers} LoRA‑wrapped modules in UNet (expecting ~230).")



try:
    pipe_base.enable_xformers_memory_efficient_attention()
    pipe_lora.enable_xformers_memory_efficient_attention()
    print("✅ xFormers enabled on both pipelines")
except Exception as e:
    print("⚠️ xFormers unavailable:", e)

print("✅ Module 3 complete: pipe_base (no LoRA), pipe_lora (LoRA-injected) ready.")

# verify LoRA wrappers
print("🔎 Checking injected LoRA modules in UNet:")
wrapped = 0
for name, mod in pipe_lora.unet.named_modules():
    if isinstance(mod, (LoRAInjectConv2d, LoRAInjectLinear)):
        wrapped += 1
        print("  • Wrapped:", name)
print(f"🔎 Found {wrapped} LoRA‑wrapped modules in UNet (expecting ~230).")



from PIL import ImageOps
import random
from PIL import Image, ImageOps

def resize_with_padding(img: Image.Image,
                        target_size: (int, int),
                        fill=(0,0,0)) -> Image.Image:
    # scale up and pad image to 512 x 516
    target_w, target_h = target_size
    orig_w, orig_h     = img.size

    scale = min(target_w / orig_w, target_h / orig_h)

    new_w = int(round(orig_w * scale))
    new_h = int(round(orig_h * scale))
    img = img.resize((new_w, new_h), Image.LANCZOS)

    pad_left   = max((target_w - new_w) // 2, 0)
    pad_top    = max((target_h - new_h) // 2, 0)
    pad_right  = max(target_w - new_w - pad_left, 0)
    pad_bottom = max(target_h - new_h - pad_top, 0)

    return ImageOps.expand(img,
                           border=(pad_left, pad_top, pad_right, pad_bottom),
                           fill=fill)



def calculate_combined_score(
    gen_img: PIL.Image.Image,
    prompt: str,
    seed: int,
    orig_image: PIL.Image.Image,
    mtcnn_detector,
    lpips_model,
    clip_model,
    clip_processor,
    config: dict,
    mask_image: PIL.Image.Image,
    box: Tuple[int,int,int,int]
) -> float:
    clip = calculate_clip_score(gen_img, prompt, clip_model, clip_processor, config["device"]) or 0.0

    orig_crop = orig_image.crop(box)
    gen_crop  = gen_img.crop(box)
    t1 = prepare_for_lpips(orig_crop, config["device"])
    t2 = prepare_for_lpips(gen_crop,  config["device"])
    if lpips_model is not None and t1 is not None and t2 is not None:
        lp = float(lpips_model(t1, t2).item())
    else:
        lp = None

    import numpy as np
    from skimage.metrics import structural_similarity as _ssim
    og = np.array(orig_crop.convert("L"))
    gg = np.array(gen_crop.convert("L"))
    if og.shape == gg.shape and og.size > 0:
        dr = og.max() - og.min() if og.max() > og.min() else 1.0
        ss = float(_ssim(og, gg, data_range=dr))
    else:
        ss = None

    land = compute_landmark_sim(np.array(orig_crop), np.array(gen_crop), config)

    w = config
    total_w = w["clip_weight"] + w["lpips_weight"] + w["ssim_weight"] + w["land_weight"]
    score = (
        clip * w["clip_weight"]
        + (1 - lp if isinstance(lp, float) else 0.0) * w["lpips_weight"]
        + (ss or 0.0) * w["ssim_weight"]
        + (land or 0.0) * w["land_weight"]
    ) / total_w

    return score


import os, glob, random
from PIL import Image
from IPython.display import display

local_folder = CONFIG["local_laion_folder"]

patterns = ["*.jpg","*.jpeg","*.png"]
image_files = []
for pat in patterns:
    image_files += glob.glob(os.path.join(local_folder, "**", pat), recursive=True)

print("Scanning →", local_folder)
print(f"✅ Found {len(image_files)} image(s). Sample:")
for p in image_files[:5]:
    print("  ", p)
if not image_files:
    raise RuntimeError(f"No images found in `{local_folder}` — make sure your unzip step ran successfully.")

test_img = random.choice(image_files)
print("\n▶️ Testing on:", test_img)
raw     = Image.open(test_img).convert("RGB")
resized = resize_with_padding(raw, target_size=(CONFIG["image_size"], CONFIG["image_size"]))
print("   Original size:", raw.size)
print("   Padded  size:", resized.size)
display(resized)


# test "resize_with_padding" function
test_img_path = random.choice(image_files)
raw_img = Image.open(test_img_path).convert("RGB")
resized_img = resize_with_padding(raw_img, target_size=(CONFIG["image_size"], CONFIG["image_size"]))

print("Original size:", raw_img.size)
print("Resized + padded size:", resized_img.size)
display(resized_img)



import numpy as np
def detect_faces_with_filter(pil_img, mtcnn, min_area_ratio=0.001, min_confidence=0.90, verbose=True):
    img_np = np.array(pil_img)
    width, height = pil_img.size
    img_area = width * height
    boxes, probs = mtcnn.detect(pil_img)

    if boxes is None or len(boxes) == 0:
        if verbose:
            print("⚠️ No faces detected.")
        return [], [], []

    kept, low_conf, too_small = [], [], []

    for i, (box, conf) in enumerate(zip(boxes, probs)):
        if conf is None or conf < min_confidence:
            if verbose:
                print(f"⚫ Face {i} skipped (low confidence: {conf:.2f})")
            low_conf.append((box, conf))
            continue

        x1, y1, x2, y2 = map(int, box)
        area = (x2 - x1) * (y2 - y1)
        if area / img_area < min_area_ratio:
            if verbose:
                print(f"⚪ Face {i} skipped (too small: {area/img_area:.2%})")
            too_small.append((box, conf))
            continue

        if verbose:
            print(f"🔴 Face {i} kept: conf={conf:.2f}, size={area/img_area:.2%}")
        kept.append((box, conf))

    if verbose:
        print(f"🧠 Total valid faces: {len(kept)}")
    return kept, low_conf, too_small


import random
import numpy as np
from PIL import Image, ImageOps, ImageDraw
from IPython.display import display

from PIL import Image, ImageDraw

def generate_elliptical_mask(pil_img, face_boxes, face_probs=None,
                              min_area_ratio=None, min_confidence=None, expand_ratio: float = 1.0):
    # Generates a binary mask with white elliptical regions over detected faces.
    if min_area_ratio is None:
        min_area_ratio = CONFIG.get("min_face_ratio", 0.001)
    if min_confidence is None:
        min_confidence = CONFIG.get("min_confidence", 0.90)

    width, height = pil_img.size
    img_area = width * height
    mask = Image.new("L", (width, height), 0)
    draw = ImageDraw.Draw(mask)

    kept_boxes = []
    for i, box in enumerate(face_boxes):
        if face_probs is not None and i < len(face_probs):
            if face_probs[i] < min_confidence:
                continue
        x1, y1, x2, y2 = map(int, box)
        face_area = (x2 - x1) * (y2 - y1)
        if face_area < min_area_ratio * img_area:
            continue
        cx, cy = (x1 + x2) / 2, (y1 + y2) / 2
        w = (x2 - x1) * expand_ratio
        h = (y2 - y1) * expand_ratio

        x1n = int(cx - w/2)
        y1n = int(cy - h/2)
        x2n = int(cx + w/2)
        y2n = int(cy + h/2)

        draw.ellipse([x1n, y1n, x2n, y2n], fill=255)
        kept_boxes.append((x1n, y1n, x2n, y2n))
    return mask, kept_boxes


# function to test "generate_elliptical_mask"
from PIL import ImageDraw
from IPython.display import display

def test_generate_elliptical_mask(image_files, mtcnn_detector, config, expand_ratio):

    test_path = random.choice(image_files)
    raw_img = Image.open(test_path).convert("RGB")
    resized_img = resize_with_padding(raw_img, target_size=(config["image_size"], config["image_size"]))

    kept, low_conf, too_small = detect_faces_with_filter(
        resized_img,
        mtcnn=mtcnn_detector,
        min_area_ratio=config.get("min_face_ratio", 0.001),
        min_confidence=config.get("min_confidence", 0.90),
        verbose=True
    )

    valid_boxes = [box for box, _ in kept]
    valid_scores = [score for _, score in kept]
    mask, final_boxes = generate_elliptical_mask(
        resized_img,
        face_boxes=valid_boxes,
        face_probs=valid_scores,
        min_area_ratio=config.get("min_face_ratio", 0.001),
        min_confidence=config.get("min_confidence", 0.90),
        expand_ratio   = 1.3
    )

    print("✅ Displaying original image with face boxes:")
    drawn = resized_img.copy()
    draw = ImageDraw.Draw(drawn)
    for box in final_boxes:
        draw.rectangle(box, outline="green", width=3)
    display(drawn)

    print("🎭 Corresponding elliptical face mask:")
    display(mask)


test_generate_elliptical_mask(image_files=image_files, mtcnn_detector=mtcnn_detector, config=CONFIG, expand_ratio = 1.3)



import mediapipe as mp
import numpy as np

def estimate_yaw_mediapipe(pil_img, face_boxes, config):
    # Estimate yaw of each face using mediapipe face mesh
    mp_face_mesh = mp.solutions.face_mesh
    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=10) as face_mesh:
        img_np = np.array(pil_img)
        results = face_mesh.process(img_np)

        if not results.multi_face_landmarks:
            return [(None, "")] * len(face_boxes)

        landmark_sets = []
        for face_landmarks in results.multi_face_landmarks:
            points = [(int(lm.x * pil_img.width), int(lm.y * pil_img.height)) for lm in face_landmarks.landmark]
            landmark_sets.append(points)

        def compute_yaw(landmarks):
            try:
                left_eye = np.array(landmarks[33])
                right_eye = np.array(landmarks[263])
                dx, dy = right_eye - left_eye
                yaw = np.degrees(np.arctan2(dy, dx))
                suffix = " side angle" if abs(yaw) > 30 else ""
                return yaw, suffix
            except:
                return None, ""

        results = []
        for yaw, suffix in (compute_yaw(lms) for lms in landmark_sets[:len(face_boxes)]):
            results.append((yaw or 0.0, 0.0, suffix or "", ""))
        return results


import matplotlib.pyplot as plt
from PIL import ImageDraw

def test_estimate_yaw(image_files, mtcnn_detector, config):
    test_path = random.choice(image_files)
    raw_img = Image.open(test_path).convert("RGB")
    resized_img = resize_with_padding(raw_img, target_size=(config["image_size"], config["image_size"]))

    kept, _, _ = detect_faces_with_filter(
        resized_img, mtcnn_detector,
        min_area_ratio=config["min_face_ratio"],
        min_confidence=config["min_confidence"],
        verbose=True
    )

    kept_boxes = [box for box, _ in kept]
    yaw_data = estimate_yaw_mediapipe(resized_img, kept_boxes, config)
    pose_data = estimate_yaw_mediapipe(resized_img, kept_boxes, config)


    drawn = resized_img.copy()
    draw = ImageDraw.Draw(drawn)
    for (x1, y1, x2, y2), (yaw, pitch, yaw_suffix, _roll) in zip(kept_boxes, pose_data):
        draw.rectangle([x1, y1, x2, y2], outline="green", width=3)
        text = f"Yaw {yaw:+.1f}°{yaw_suffix}" if yaw is not None else "Yaw: Unknown"
        draw.text((x1, y1 - 10), text, fill="white")

    print(f"🧠 Yaw estimation complete for {len(kept_boxes)} face(s).")
    display(drawn)

# test "estimate_yaw_mediapipe"
test_estimate_yaw(image_files=image_files, mtcnn_detector=mtcnn_detector, config=CONFIG)


def generate_dynamic_prompts(
    yaw_data,
    age_groups,
    base_prompt="realistic human face",
    extra_tags="highly detailed, intricate details, sharp focus, ultra-detailed skin texture, masterpiece, best quality",
    negative_tags="deformed, cartoon, illustration, painting, drawing, sketch, anime, disfigured, ugly, blurry, low quality, noisy, text, signature, watermark, multiple faces, extra limbs, fused fingers, bad anatomy, worst quality, normal quality"
):
    # generate a list of positive and negative prompts for each detected face.

    prompts = []
    for i in range(len(yaw_data)):
        _, yaw_suffix = yaw_data[i] # ex. "side angle" or ""
        age = age_groups[i]         # ex. "young adult"

        # create full prompt - yaw and age included
        positive_prompt = f"{base_prompt}, {age}{yaw_suffix}, {extra_tags}"
        negative_prompt = negative_tags.strip()

        prompts.append((positive_prompt, negative_prompt))
    return prompts


def test_prompt_generation_pipeline(image_files, mtcnn_detector, config):
    import random
    from PIL import ImageDraw

    test_path   = random.choice(image_files)
    raw_img     = Image.open(test_path).convert("RGB")
    resized_img = resize_with_padding(raw_img, target_size=(config["image_size"], config["image_size"]))

    kept, _, _ = detect_faces_with_filter(
        resized_img, mtcnn_detector,
        min_area_ratio=config["min_face_ratio"],
        min_confidence=config["min_confidence"],
        verbose=True
    )
    kept_boxes = [box for box, _ in kept]
    if not kept_boxes:
        print("❌ No valid faces to generate prompts.")
        return

    # normalise yaw_data to (yaw, suffix)
    raw_pose_data = estimate_yaw_mediapipe(resized_img, kept_boxes, config)
    yaw_data = []
    for item in raw_pose_data:
        if len(item) == 2:
            yaw, suffix = item
        else:
            yaw, _, suffix, _ = item
        yaw_data.append((yaw, suffix))

    # age_estimates
    age_groups = []
    for (x1,y1,x2,y2) in kept_boxes:
        crop = resized_img.crop((x1, y1, x2, y2))
        age_groups.append(estimate_age(crop))

    # generate (pos, neg) prompts
    prompts = generate_dynamic_prompts(yaw_data, age_groups)

    # draw
    drawn = resized_img.copy()
    draw  = ImageDraw.Draw(drawn)
    for (x1, y1, x2, y2), (pos_prompt, neg_prompt) in zip(kept_boxes, prompts):
        draw.rectangle([x1, y1, x2, y2], outline="blue", width=2)
        draw.text((x1, y2 + 5), pos_prompt[:50] + "...", fill="white")

    print(f"generated prompts for {len(prompts)} face(s).")
    for pos_prompt, neg_prompt in prompts:
        print(pos_prompt)
    display(drawn)


test_prompt_generation_pipeline(image_files=image_files, mtcnn_detector=mtcnn_detector, config=CONFIG)

import logging
import os
import random
from itertools import product
from typing import List, Tuple, Union, Dict, Optional


import cv2
import numpy as np
import torch
from PIL import Image, ImageDraw, ImageOps

try:
    import lpips
except ImportError:
    lpips = None
    logging.warning("lpips library not found. LPIPS calculations will be skipped.")
try:
    from transformers import CLIPProcessor, CLIPModel
except ImportError:
    CLIPProcessor = None
    CLIPModel = None
    logging.warning(
        "transformers library not found. CLIP calculations will be skipped."
    )
try:
    from facenet_pytorch import MTCNN
except ImportError:
    MTCNN = None
    logging.warning(
        "facenet_pytorch library not found. "
        "MTCNN detector cannot be initialised by default."
    )
try:
    import mediapipe as mp

    mp_face_mesh = mp.solutions.face_mesh
    mp_drawing = mp.solutions.drawing_utils
    mp_drawing_styles = mp.solutions.drawing_styles

except ImportError:
    mp = None
    mp_face_mesh = None
    logging.error(
        "mediapipe library not found. "
        "Pose estimation and landmark features will be disabled."
    )
try:
    from skimage.metrics import structural_similarity as ssim
except ImportError:
    ssim = None
    logging.warning(
        "scikit-image not found. SSIM calculations will be skipped."
    )


# logging config
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

# initialise mediapipe facemesh
face_mesh = None
if mp_face_mesh:
    try:
        face_mesh = mp_face_mesh.FaceMesh(
            static_image_mode=True,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
        )
        logging.info(
            "mediapipe facemesh initialised successfully "
            "(with landmark refinement)."
        )
    except Exception as e:
        logging.error(f" :((  Failed to initialise mediapipe facemesh: {e}")
        face_mesh = None
else:
    logging.error(
        "mesdiapipe facemesh cannot be initialised because "
        "mediapipe library is not available."
    )

lpips_model = None
if lpips:
    try:
        if "CONFIG" in locals() and isinstance(CONFIG, dict) and "device" in CONFIG:
            logging.info(
                f"🔄 Loading LPIPS model (vgg) to device: {CONFIG['device']}..."
            )
            lpips_model = lpips.LPIPS(net="vgg").to(CONFIG["device"])
            lpips_model.eval()
            logging.info("✅ LPIPS model loaded successfully.")
        else:
            logging.warning(
                "⚠️ LPIPS model loading skipped: CONFIG or device key not defined."
            )
    except Exception as e:
        logging.warning(f"⚠️ Could not load LPIPS model: {e}")
        lpips_model = None
else:
    logging.warning("⚠️ LPIPS model skipped: lpips library not available.")

clip_model_name = "openai/clip-vit-large-patch14"
clip_model = None
clip_processor = None
if CLIPModel and CLIPProcessor:
    try:
        if "CONFIG" in locals() and isinstance(CONFIG, dict) and "device" in CONFIG:
            logging.info(f"🔄 Loading CLIP model: {clip_model_name}...")
            clip_model = CLIPModel.from_pretrained(clip_model_name).to(
                CONFIG["device"]
            )
            clip_processor = CLIPProcessor.from_pretrained(clip_model_name)
            clip_model.eval()
            logging.info("✅ CLIP model and processor loaded successfully.")
        else:
            logging.warning(
                "⚠️ CLIP model/processor loading skipped: "
                "CONFIG or device key not defined."
            )
    except Exception as e:
        logging.warning(f"⚠️ Could not load CLIP model/processor: {e}")
        clip_model = None
        clip_processor = None
else:
    logging.warning(
        "⚠️ CLIP model/processor skipped: transformers library not available."
    )

@torch.no_grad()
def calculate_clip_score(
    image_pil: Optional[Image.Image],
    prompt_text: Optional[str],
    model: Optional[CLIPModel],
    processor: Optional[CLIPProcessor],
    device: str,
) -> Optional[float]:
    # Calculates a CLIP score between a single PIL image and a text prompt.
    if not all([image_pil, prompt_text, model, processor]):
        logging.debug("CLIP score skipped: missing inputs.")
        return None
    prompt = str(prompt_text)
    try:
        encoding = processor(
            text=[prompt],
            images=[image_pil],
            return_tensors="pt",
            padding=True,
            truncation=True,
        )
        for k, v in encoding.items():
            encoding[k] = v.to(device)
        img_feats = model.get_image_features(
            pixel_values=encoding["pixel_values"]
        )
        txt_feats = model.get_text_features(
            input_ids=encoding["input_ids"],
            attention_mask=encoding["attention_mask"],
        )
        img_feats = img_feats / img_feats.norm(dim=-1, keepdim=True)
        txt_feats = txt_feats / txt_feats.norm(dim=-1, keepdim=True)
        similarity = (img_feats * txt_feats).sum(dim=-1)
        score = float(
            torch.clamp(100 * similarity, min=0.0, max=100.0).item()
        )
        return score
    except Exception as e:
        logging.error(f"CLIP score calculation error (prompt={prompt!r}): {e}")
        return None


# LPIPS helpder
def prepare_for_lpips(
    img_pil: Optional[Image.Image], device: str
) -> Optional[torch.Tensor]:
    # Converts PIL image to PyTorch tensor normalized for LPIPS.
    if img_pil is None:
        return None
    transform = transforms.Compose(
         [
             transforms.ToTensor(),
             transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
         ]

    )
    try:
        return transform(img_pil).unsqueeze(0).to(device)
    except Exception as e:
        logging.error(f"Error preparing image for LPIPS: {e}")
        return None



def fmt_metric(val: Optional[Union[float, int]]) -> str:
    """Formats metric value to string 'N/A' or '0.XXXf'."""
    return f"{val:.3f}" if isinstance(val, (int, float)) else "N/A"


# pose estimate
def estimate_pose_mediapipe(
    image_pil: Image.Image,
    boxes: List[Tuple[float, float, float, float]],
    config: dict,
) -> List[Tuple[float, float, str, str]]:
    # estimates head pose (yaw, pitch) and generates descriptive suffixes
    if face_mesh is None:
        logging.warning(
            "mediapipe facemesh unavailable. "
            "resturning default pose (0, 0, '', '') for all faces."
        )
        return [(0.0, 0.0, "", "")] * len(boxes)

    img_np_rgb = np.array(image_pil)
    img_h, img_w = img_np_rgb.shape[:2]
    pose_results = []
    yaw_thresh = config.get("pose_yaw_threshold", 15.0)
    pitch_thresh = config.get("pose_pitch_threshold", 10.0)

    # canon face model points based on mediapipe landmarks
    face_3d_model_points = np.array(
        [
            [0.0, 0.0, 0.0],  # nose tip (1)
            [0.0, -330.0, -65.0],  # chin (152)
            [-225.0, 170.0, -135.0],  # left eye left corner (263)
            [225.0, 170.0, -135.0],  # right eye right corner (33)
            [-150.0, -150.0, -125.0],  # left Mouth corner (61)
            [150.0, -150.0, -125.0],  # right mouth corner (291)
        ],
        dtype=np.float64,
    )

    focal_length = img_w
    center = (img_w / 2, img_h / 2)
    camera_matrix = np.array(
        [[focal_length, 0, center[0]], [0, focal_length, center[1]], [0, 0, 1]],
        dtype=np.float64,
    )
    dist_coeffs = np.zeros((4, 1))

    for i, box in enumerate(boxes):
        yaw_angle, pitch_angle = 0.0, 0.0
        yaw_suffix, pitch_suffix = "", ""
        try:
            x1, y1, x2, y2 = map(int, box)
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(img_w, x2), min(img_h, y2)
            if x1 >= x2 or y1 >= y2:
                logging.warning(f"Skipping pose for invalid box {i}: {box}")
                pose_results.append((0.0, 0.0, "", ""))
                continue

            face_crop_np = img_np_rgb[y1:y2, x1:x2]
            if face_crop_np.size == 0:
                logging.warning(f"Skipping pose for empty crop from box {i}: {box}")
                pose_results.append((0.0, 0.0, "", ""))
                continue

            face_crop_bgr = cv2.cvtColor(face_crop_np, cv2.COLOR_RGB2BGR)
            mp_results = face_mesh.process(face_crop_bgr)

            if mp_results.multi_face_landmarks:
                landmarks = mp_results.multi_face_landmarks[0].landmark
                required_indices = [1, 152, 263, 33, 61, 291]

                if all(idx < len(landmarks) for idx in required_indices):
                    face_2d_image_points = np.array(
                        [
                            (
                                landmarks[idx].x * face_crop_np.shape[1],
                                landmarks[idx].y * face_crop_np.shape[0],
                            )
                            for idx in required_indices
                        ],
                        dtype=np.float64,
                    )

                    success, rotation_vector, _ = cv2.solvePnP(
                        face_3d_model_points,
                        face_2d_image_points,
                        camera_matrix,
                        dist_coeffs,
                        flags=cv2.SOLVEPNP_ITERATIVE,
                    )

                    if success:
                        rotation_matrix, _ = cv2.Rodrigues(rotation_vector)
                        angles, _, _, _, _, _ = cv2.RQDecomp3x3(rotation_matrix)
                        pitch_angle, yaw_angle = angles[0], angles[1]

                        if yaw_angle > yaw_thresh:
                            yaw_suffix = " facing right"
                        elif yaw_angle < -yaw_thresh:
                            yaw_suffix = " facing left"
                        if pitch_angle > pitch_thresh:
                            pitch_suffix = " looking down"
                        elif pitch_angle < -pitch_thresh:
                            pitch_suffix = " looking up"
                else:
                    logging.debug(
                        f" required landmarks missing for pose estimation in face {i}."
                    )
            else:
                logging.debug(
                    f"no landmarks found by mediapipe for face {i} in crop."
                )
        except Exception as e_pose:
            logging.warning(f"error estimating pose for face {i}: {e_pose}")

        pose_results.append((yaw_angle, pitch_angle, yaw_suffix, pitch_suffix))
    return pose_results


def extract_visible_landmarks(
    img_np_rgb: np.ndarray, vis_thresh: float = 0.5
) -> List[Tuple[float, float]]:
    # extracts visible facial landmarks
    if face_mesh is None:
        return []
    try:
        if not isinstance(img_np_rgb, np.ndarray):
            img_np_rgb = np.array(img_np_rgb)
        # ensure image has 3 dimensions (H, W, C)
        if img_np_rgb.ndim != 3 or img_np_rgb.shape[2] not in [3, 4]:
             logging.warning(f"Unexpected image shape for landmark extraction: {img_np_rgb.shape}")
             return []
        # convert to BGR
        if img_np_rgb.shape[2] == 3:
            image_bgr = cv2.cvtColor(img_np_rgb, cv2.COLOR_RGB2BGR)
        else:
            image_bgr = cv2.cvtColor(img_np_rgb, cv2.COLOR_RGBA2BGR)

        results = face_mesh.process(image_bgr)
        if not results.multi_face_landmarks:
            return []
        landmarks = results.multi_face_landmarks[0].landmark
        return [
            (lm.x, lm.y)
            for lm in landmarks
            if getattr(lm, "visibility", 1.0) >= vis_thresh
        ]
    except Exception as e:
        logging.error(f"Error during landmark extraction: {e}")
        return []


def compute_landmark_sim(
    orig_np_rgb: np.ndarray, gen_np_rgb: np.ndarray, config: dict
) -> Optional[float]:
    # Computes landmark similarity score (0-1, higher is better).
    min_pts = config.get("landmark_min_pts", 10)
    vis_thresh = config.get("landmark_vis_threshold", 0.5)
    o_pts = extract_visible_landmarks(orig_np_rgb, vis_thresh)
    g_pts = extract_visible_landmarks(gen_np_rgb, vis_thresh)

    if len(o_pts) < min_pts or len(o_pts) != len(g_pts):
        return None
    try:
        dists = [
            np.hypot(ox - gx, oy - gy)
            for (ox, oy), (gx, gy) in zip(o_pts, g_pts)
        ]
        mean_dist = sum(dists) / len(dists)
        o_x = [x for x, _ in o_pts]
        face_width = max(o_x) - min(o_x) + 1e-6
        return max(0.0, 1.0 - (mean_dist / face_width))
    except Exception as e:
        logging.error(f"error calculating landmark similarity: {e}")
        return None


def minmax(arr: List[Optional[float]]) -> List[Optional[float]]:
    # apply min-max normalisation to list [0, 1]
    valid_arr = [x for x in arr if isinstance(x, (int, float))]
    if not valid_arr:
        return arr
    if len(set(valid_arr)) == 1:
        return [0.5 if isinstance(x, (int, float)) else None for x in arr]

    mn, mx = min(valid_arr), max(valid_arr)
    denominator = mx - mn + 1e-6
    normalized = [
        (x - mn) / denominator if isinstance(x, (int, float)) else None
        for x in arr
    ]
    return [
        max(0.0, min(1.0, x)) if isinstance(x, (int, float)) else None
        for x in normalized
    ]

# for too many faces
from operator import itemgetter
@torch.no_grad()
def run_module_custom_prompts(
    image_pil: Image.Image,
    face_masks: List[Image.Image],
    boxes: List[Tuple[float, float, float, float]],
    age_labels: List[str],
    yaw_data: List[Tuple[Optional[float], str]],
    pipe_lora,
    mtcnn_detector,
    lpips_model,
    clip_model,
    clip_processor,
    config: dict,
    base_extra_tags: str,
    base_neg_prompt: str,
) -> Dict[int, Dict[str, Optional[dict]]]:
    # for each face, generate 4 candidates per cond, score and normalise.
    # pick best, chain, rerank if shiny, and return results
    device = config["device"]
    guidance = config["guidance_scale"]
    steps = config["num_inference_steps"]
    N = config["num_candidates"]
    K = config.get("rerank_shiny_candidates", 3)
    shine_th = config.get("shiny_highlight_threshold", 200)
    pratio = config.get("shiny_pixel_ratio", 0.02)
    land_yaw_thresh = config.get("landmark_yaw_threshold", 100.0)
    gen_batch = config.get("generation_batch_size", N)

    weights = {
        "clip": config.get("clip_weight", 0.4),
        "lpips": config.get("lpips_weight", 0.2),
        "ssim": config.get("ssim_weight", 0.2),
        "land": config.get("land_weight", 0.2),
    }

    S = config["image_size"]
    orig_image = image_pil.resize((S, S), Image.LANCZOS).convert("RGB")
    canvas = orig_image.copy()

    # all race/gender conditions with highest FID
    races   = ["asian","caucasian","african","middle eastern"]
    genders = ["male","female"]
    cond_keys = [f"{r}_{g}" for r in races for g in genders]
    cond_txts = [f"{r} {g}"       for r in races for g in genders]

    results = {}

    for i, mask in enumerate(face_masks):
        if i >= len(boxes) or i >= len(age_labels) or i >= len(yaw_data):
            continue

        age = age_labels[i]
        yaw_angle, yaw_suf = yaw_data[i]
        m = mask.resize(orig_image.size, Image.NEAREST).convert("L")

        x1,y1,x2,y2 = map(int, boxes[i])
        crop_orig = orig_image.crop((x1,y1,x2,y2))
        crop_np = np.array(crop_orig)

        best_per_cond = {}

        for ci, cond_key in enumerate(cond_keys):
            txt = cond_txts[ci]
            prompt_cond = f"{age} {txt}{yaw_suf}, {base_extra_tags}"
            neg_cond    = base_neg_prompt

            seeds = [config["seed"] + j for j in range(N)]
            gens  = [torch.Generator(device=device).manual_seed(s) for s in seeds]

            all_imgs = []
            for idxs in batchify(list(range(N)), gen_batch):
                ps = [prompt_cond]*len(idxs)
                ns = [neg_cond]*len(idxs)
                gs = [gens[j] for j in idxs]
                out = pipe_lora(
                    prompt=ps,
                    negative_prompt=ns,
                    image=canvas,
                    mask_image=m,
                    guidance_scale=guidance,
                    num_inference_steps=steps,
                    generator=gs,
                    inpaint_full_res=True,
                    mask_blur=8,
                )
                all_imgs.extend(out.images if isinstance(out.images, list) else [out.images])

            cands = []
            for j, img_out in enumerate(all_imgs):
                metrics = {
                    "img": img_out,
                    "seed": seeds[j],
                    "clip": None, "lpips": None,
                    "ssim": None, "land": None,
                    "combined": -1.0,
                    "is_shiny_rerun": False,
                    "face_conf": 0.0,
                }
                if img_out is None:
                    cands.append(metrics)
                    continue


                try:
                    metrics["clip"] = calculate_clip_score(
                        img_out, prompt_cond, clip_model, clip_processor, device
                    )
                except:
                    pass

                gen_crop = img_out.crop((x1,y1,x2,y2))
                try:
                    t1 = prepare_for_lpips(crop_orig, device)
                    t2 = prepare_for_lpips(gen_crop, device)
                    if lpips_model and t1 is not None and t2 is not None:
                        metrics["lpips"] = float(lpips_model(t1, t2).item())
                except:
                    pass

                try:
                    og = np.array(crop_orig.convert("L"))
                    gg = np.array(gen_crop.convert("L"))
                    if og.shape == gg.shape:
                        dr = og.max()-og.min() or 1.0
                        ws = min(7, min(og.shape))
                        ws = ws//2*2+1 if ws>=3 else 3
                        metrics["ssim"] = float(ssim(og, gg, data_range=dr, win_size=ws))
                except:
                    pass

                if isinstance(yaw_angle, (int,float)) and abs(yaw_angle) <= land_yaw_thresh:
                    try:
                        gen_np = np.array(gen_crop)
                        metrics["land"] = compute_landmark_sim(crop_np, gen_np, config)
                    except:
                        pass

                cands.append(metrics)

            # normalise
            def norm(key):
                vals = [c[key] for c in cands]
                nm   = minmax(vals)
                return nm
            clip_n  = norm("clip")
            lpips_n = [(1-v) if isinstance(v,float) else None for v in norm("lpips")]
            ssim_n  = norm("ssim")
            land_n  = norm("land")

            # combined score
            for idx, c in enumerate(cands):
                scores = {
                    "clip": clip_n[idx],
                    "lpips": lpips_n[idx],
                    "ssim": ssim_n[idx],
                    "land": land_n[idx],
                }
                # pick available
                avail = {m:s for m,s in scores.items() if isinstance(s,float)}
                if not avail:
                    continue
                w = {m:weights[m] for m in avail}
                tot = sum(w.values())+1e-6
                c["combined"] = sum((w[m]/tot)*avail[m] for m in avail)

            # select best
            valid = [c for c in cands if c["img"] and c["combined"]>=0]
            best = max(valid, key=lambda x: x["combined"]) if valid else None

            if best:
                patch = best["img"].resize(orig_image.size, Image.LANCZOS)
                canvas = Image.composite(patch, canvas, m)
                if is_shiny_patch(patch, (x1,y1,x2,y2), shine_th, pratio):
                    topk = sorted(cands, key=lambda x: x["combined"], reverse=True)[:K]
                    rerank = []
                    for rc in topk:
                        out2 = pipe_lora(
                            prompt=[prompt_cond],
                            negative_prompt=[neg_cond],
                            image=canvas,
                            mask_image=m,
                            guidance_scale=guidance,
                            num_inference_steps=steps,
                            generator=[torch.Generator(device=device).manual_seed(rc["seed"])],
                            inpaint_full_res=True, mask_blur=8,
                        )
                        rerank.append((rc["seed"], out2.images[0]))
                    best["is_shiny_rerun"] = True

                try:
                    _, probs = mtcnn_detector.detect(np.array(best["img"].crop((x1,y1,x2,y2))))
                    best["face_conf"] = float(np.max(probs)) if probs is not None else 0.0
                except:
                    pass

            best_per_cond[cond_key] = best

        results[i] = best_per_cond

    return results

import os
import glob
import logging
import zipfile
import random
from itertools import product
from tqdm.notebook import tqdm
from PIL import Image, ImageDraw
import torch

def inpaint_and_upload_faces(
    config: dict,
    pipe_lora,
    mtcnn_detector,
    estimate_age_func,
    estimate_pose_func,
    resize_func,
    detect_filter_func,
    run_module_custom_prompts,
    clip_model,
    clip_processor,
    lpips_model,
):
    out_src  = config["local_laion_folder"]
    out_dir  = config["local_output_folder"]
    halfdone = config.get("halfdone_zip")
    S        = config["image_size"]
    os.makedirs(out_dir, exist_ok=True)

    import re

    # first check for "halfdone_zip", unzip and check what has been inpainted if so
    done_bases = set()
    if halfdone and os.path.isfile(halfdone):
        logging.info(f"Resuming from partial zip: {halfdone}")
        with zipfile.ZipFile(halfdone, "r") as zf:
            zf.extractall(out_dir)

            for member in zf.namelist():
                if member.endswith("/"):
                    continue
                fn   = os.path.basename(member)
                stem = os.path.splitext(fn)[0]
                m = re.match(r".*?(\d+)(?:_face\d+_|_combo\d+_)", stem)
                if m:
                    done_bases.add(m.group(1))
    else:
        done_bases = set()

    all_inputs = sorted(glob.glob(f"{out_src}/**/*.*", recursive=True))
    to_do = [
        p for p in all_inputs
        if p.lower().endswith((".jpg","png","jpeg"))
          and os.path.splitext(os.path.basename(p))[0] not in done_bases
    ]

    print(f"{len(to_do)} images to process")


    # start inpainting
    for img_path in tqdm(to_do, desc="Batch Inpainting"):
        try:
            base = os.path.splitext(os.path.basename(img_path))[0]
            img  = Image.open(img_path).convert("RGB")
            img  = resize_func(img, (S, S))
            # detect and filter
            kept,_,_ = detect_filter_func(img, mtcnn_detector,
                                          config["min_face_ratio"],
                                          config["min_confidence"],
                                          verbose=False)
            if not kept or len(kept)>4:
                print(f"Skipping {base}: {len(kept)} faces")
                continue

            # sort faces by area
            boxes = [b for b,_ in kept]
            areas = [(i,(b[2]-b[0])*(b[3]-b[1])) for i,b in enumerate(boxes)]
            sorted_idxs = [i for i,_ in sorted(areas, key=lambda x:x[1], reverse=True)]
            largest_idx = sorted_idxs[0]
            other_idxs  = sorted_idxs[1:]

            # build masks, ages, yaw
            masks = []; ages = []
            for box,_ in kept:
                x1,y1,x2,y2 = map(int,box)
                m = Image.new("L",(S,S),0)
                draw = ImageDraw.Draw(m)
                w,h = (x2-x1)*config.get("mask_expand_ratio",1.3),\
                      (y2-y1)*config.get("mask_expand_ratio",1.3)
                cx,cy = (x1+x2)/2,(y1+y2)/2
                draw.ellipse([cx-w/2,cy-h/2, cx+w/2,cy+h/2],fill=255)
                masks.append(m)
                ages.append(estimate_age_func(img.crop((x1,y1,x2,y2))) or "adult")

            # yaw
            pose = estimate_pose_func(img, boxes, config)
            yaw_data = [(y,suf) for y,_,suf,_ in pose]

            # build per-face 8 prompts
            base_p = config.get("base_prompt","realistic human face")
            extra  = config.get("base_extra_tags","")
            neg    = config.get("base_neg_prompt","")
            races  = ["asian","caucasian","african","middle eastern"]
            genders= ["male","female"]
            conds  = [f"{r} {g}" for r in races for g in genders]

            face_prompts = []
            for i in range(len(boxes)):
                age,suf = ages[i], yaw_data[i][1]
                face_prompts.append([
                    (f"{base_p}, {age} {c}{suf}, {extra}", neg)
                    for c in conds
                ])

            # per-face reranking: pick best seed per prompt
            reranked_imgs   = {}
            reranked_scores = {}
            for i, prompts in enumerate(face_prompts):
                # for largest face -> keep all 8, for others -> keep 2
                if i == largest_idx:
                    use = prompts
                else:
                    use = random.sample(prompts, min(2, len(prompts)))
                best_imgs, best_scs = [], []
                for pos,ng in use:
                    best_sc, best_img = -1e9, None
                    for s in range(4):
                        seed = config["seed"]+s
                        out = pipe_lora(
                            prompt=[pos], negative_prompt=[ng],
                            image=img, mask_image=masks[i],
                            guidance_scale=config["guidance_scale"],
                            num_inference_steps=config["num_inference_steps"],
                            generator=[torch.Generator(device=config["device"]).manual_seed(seed)],
                            inpaint_full_res=True, mask_blur=8
                        )
                        im = out.images[0] if isinstance(out.images,list) else out.images
                        sc = calculate_combined_score(
                            im, pos, seed,
                            img, mtcnn_detector,
                            lpips_model, clip_model, clip_processor,
                            config, masks[i], boxes[i]
                        )
                        if sc > best_sc:
                            best_sc, best_img = sc, im
                    best_imgs.append(best_img)
                    best_scs.append(best_sc)
                reranked_imgs[i]   = best_imgs
                reranked_scores[i] = best_scs

            # build every combination
            face_ids   = sorted(reranked_imgs.keys())
            combos     = list(product(*(reranked_imgs[i] for i in face_ids)))
            combo_scs  = list(product(*(reranked_scores[i] for i in face_ids)))

            final_cands = []
            for imgs, scs in zip(combos, combo_scs):
                canvas = img.copy()
                for fid, face_im in zip(face_ids, imgs):
                    canvas = Image.composite(face_im.resize((S,S)), canvas, masks[fid])
                total = sum(scs)
                final_cands.append((canvas, total))

            # sort & save top 16 (or all output imgs)
            final_cands.sort(key=lambda x:x[1], reverse=True)
            K = min(len(final_cands), 16)
            for rank, (out_im, score) in enumerate(final_cands[:K], 1):
                fn = f"{base}_combo{rank:02d}_sc{score:.3f}.png"
                out_im.save(os.path.join(out_dir, fn))

        except Exception:
            print(f"Error processing {img_path}")

    print("Batch inpainting complete.")
    return True

success = inpaint_and_upload_faces(
    config=CONFIG,
    pipe_lora=pipe_lora,
    mtcnn_detector=mtcnn_detector,
    estimate_age_func=estimate_age,
    estimate_pose_func=estimate_pose_mediapipe,
    resize_func=resize_with_padding,
    detect_filter_func=detect_faces_with_filter,
    run_module_custom_prompts=None,
    clip_model=clip_model,
    clip_processor=clip_processor,
    lpips_model=lpips_model,
)

from pathlib import Path

out = Path("/content/inpainted_results_output")
files = list(out.rglob("*.*"))
print(f"✅ Found {len(files)} files in {out!r}")
total_size = sum(f.stat().st_size for f in files)
print(f"📦 Total size: {total_size/1024**2:.2f} MB")
for f in files[:10]:
    print("  ", f)

import os
import zipfile

# create zip of inpainted outputs
output_folder = "/content/inpainted_results_output" # your folder of inpainted images
zip_path       = "/content/halfdone_output_snippet_1_1.zip"


with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
    for root, _, files in os.walk(output_folder):
        for fname in files:
            full_path = os.path.join(root, fname)
            rel_path  = os.path.relpath(full_path, output_folder)
            zf.write(full_path, arcname=rel_path)

print(f"✅ Zipped all files to {zip_path}")

# download
from google.colab import files
files.download(zip_path)